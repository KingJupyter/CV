{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PacktPublishing/Modern-Computer-Vision-with-PyTorch-2E/blob/main/Chapter15/self-attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "%pip install torch-snippets lovely-tensors pysnooper"
      ],
      "metadata": {
        "id": "lqAsmf9VMDYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30veMESsMAyb"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "from torch_snippets import *\n",
        "from pysnooper import snoop\n",
        "from builtins import print"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "\n",
        "        # Query, Key, Value projections\n",
        "        self.query = nn.Linear(embed_size, embed_size)\n",
        "        self.key = nn.Linear(embed_size, embed_size)\n",
        "        self.value = nn.Linear(embed_size, embed_size)\n",
        "\n",
        "    @snoop()\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len, embed_size)\n",
        "        query = self.query(x)  # shape: (batch_size, seq_len, embed_size)\n",
        "        key = self.key(x)      # shape: (batch_size, seq_len, embed_size)\n",
        "        value = self.value(x)  # shape: (batch_size, seq_len, embed_size)\n",
        "\n",
        "        # Compute the attention scores\n",
        "        # query shape: (batch_size, seq_len, embed_size)\n",
        "        # key shape: (batch_size, seq_len, embed_size)\n",
        "        # scores shape: (batch_size, seq_len, seq_len)\n",
        "        scores = torch.bmm(query, key.transpose(1, 2)) / (self.embed_size ** 0.5)\n",
        "\n",
        "        # Apply softmax to get the attention weights\n",
        "        # dim=-1 ensures softmax is applied across the sequence length\n",
        "        weights = F.softmax(scores, dim=-1)\n",
        "\n",
        "        # Apply the attention weights to the values\n",
        "        out = torch.bmm(weights, value)  # shape: (batch_size, seq_len, embed_size)\n",
        "        return out\n",
        "\n",
        "SA = SelfAttention(64)\n",
        "x = torch.randn(5, 3, 64)\n",
        "SA(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3ZRxWmnSlyw",
        "outputId": "e834650d-a0a4-4c07-ed03-8c41005aa98d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m\u001b[2mSource path:... \u001b[22m<ipython-input-18-b5025a279c6a>\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mself = SelfAttention(  (query): Linear(in_features=64, ...near(in_features=64, out_features=64, bias=True))\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mx = tensor[5, 3, 64] n=960 (3.8Kb) x∈[-3.357, 2.783] μ=-0.029 σ=0.997\u001b[0m\n",
            "\u001b[2m10:44:59.383754 call        13\u001b[0m     def forward(self, x):\n",
            "\u001b[2m10:44:59.386178 line        15\u001b[0m         query = self.query(x)  # shape: (batch_size, seq_len, embed_size)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mquery = tensor[5, 3, 64] n=960 (3.8Kb) x∈[-1.923, 1.820] μ=0.003 σ=0.608 grad ViewBackward0\u001b[0m\n",
            "\u001b[2m10:44:59.387399 line        16\u001b[0m         key = self.key(x)      # shape: (batch_size, seq_len, embed_size)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mkey = tensor[5, 3, 64] n=960 (3.8Kb) x∈[-1.557, 1.585] μ=-0.019 σ=0.571 grad ViewBackward0\u001b[0m\n",
            "\u001b[2m10:44:59.388865 line        17\u001b[0m         value = self.value(x)  # shape: (batch_size, seq_len, embed_size)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mvalue = tensor[5, 3, 64] n=960 (3.8Kb) x∈[-1.971, 1.636] μ=-0.012 σ=0.571 grad ViewBackward0\u001b[0m\n",
            "\u001b[2m10:44:59.390754 line        23\u001b[0m         scores = torch.bmm(query, key.transpose(1, 2)) / (self.embed_size ** 0.5)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mscores = tensor[5, 3, 3] n=45 x∈[-0.952, 0.711] μ=-0.019 σ=0.358 grad DivBackward0\u001b[0m\n",
            "\u001b[2m10:44:59.393300 line        27\u001b[0m         weights = F.softmax(scores, dim=-1)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mweights = tensor[5, 3, 3] n=45 x∈[0.135, 0.501] μ=0.333 σ=0.092 grad SoftmaxBackward0\u001b[0m\n",
            "\u001b[2m10:44:59.396021 line        30\u001b[0m         out = torch.bmm(weights, value)  # shape: (batch_size, seq_len, embed_size)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mout = tensor[5, 3, 64] n=960 (3.8Kb) x∈[-1.309, 1.075] μ=-0.013 σ=0.333 grad BmmBackward0\u001b[0m\n",
            "\u001b[2m10:44:59.398863 line        31\u001b[0m         return out\n",
            "\u001b[2m10:44:59.401625 return      31\u001b[0m         return out\n",
            "\u001b[36m\u001b[2mReturn value:.. \u001b[22mtensor[5, 3, 64] n=960 (3.8Kb) x∈[-1.309, 1.075] μ=-0.013 σ=0.333 grad BmmBackward0\u001b[0m\n",
            "\u001b[33m\u001b[2mElapsed time: \u001b[22m00:00:00.022421\u001b[0m\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor[5, 3, 64] n=960 (3.8Kb) x∈[-1.309, 1.075] μ=-0.013 σ=0.333 grad BmmBackward0"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Actual Implementation in pytorch with multihead"
      ],
      "metadata": {
        "id": "23wYhisLSmSG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ??F.multi_head_attention_forward\n",
        "# !ln -s /usr/local/lib/python3.10/dist-packages/torch/nn/functional.py .\n",
        "# Add snoop() to F.multi_head_attention_forward in above mentioned python file and run the code below"
      ],
      "metadata": {
        "id": "_Y3bxkWvMvHG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class TransformerEncoderModule(nn.Module):\n",
        "    def __init__(self, embed_size, num_heads, dropout_rate=0.1):\n",
        "        super(TransformerEncoderModule, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(embed_size)\n",
        "        self.multi_head_attention = nn.MultiheadAttention(embed_dim=embed_size, num_heads=num_heads)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(embed_size, embed_size * 4),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(embed_size * 4, embed_size),\n",
        "            nn.Dropout(dropout_rate)\n",
        "        )\n",
        "\n",
        "    def forward(self, src):\n",
        "        # Normalize and compute self-attention\n",
        "        src = self.layer_norm(src)\n",
        "        attention_output, _ = self.multi_head_attention(src, src, src)\n",
        "        src = src + self.dropout(attention_output)\n",
        "\n",
        "        # Apply feed-forward network\n",
        "        src = self.layer_norm(src)\n",
        "        feed_forward_output = self.feed_forward(src)\n",
        "        src = src + self.dropout(feed_forward_output)\n",
        "        return src\n",
        "\n",
        "# Parameters\n",
        "embed_size = 512  # Embedding size\n",
        "num_heads = 8     # Number of attention heads (ensure embed_size % num_heads == 0)\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Create the transformer encoder module\n",
        "transformer_encoder = TransformerEncoderModule(embed_size, num_heads, dropout_rate)\n",
        "\n",
        "# Example input (Batch size x Time steps x Embedding size)\n",
        "input_tensor = torch.randn(5, 3, 512)  # 1 batch, 3 time steps, 512 embeddings each\n",
        "\n",
        "# Forward pass through the transformer encoder\n",
        "output_tensor = transformer_encoder(input_tensor)\n",
        "\n",
        "print(output_tensor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BUrCVqbEMGs6",
        "outputId": "58b2be7c-1b54-474e-bed2-97db0bf5a453"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mquery = tensor[5, 3, 512] n=7680 (30Kb) x∈[-3.828, 3.956] μ=-9.934e-10 σ=1.000 grad NativeLayerNormBackward0\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mkey = tensor[5, 3, 512] n=7680 (30Kb) x∈[-3.828, 3.956] μ=-9.934e-10 σ=1.000 grad NativeLayerNormBackward0\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mvalue = tensor[5, 3, 512] n=7680 (30Kb) x∈[-3.828, 3.956] μ=-9.934e-10 σ=1.000 grad NativeLayerNormBackward0\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22membed_dim_to_check = 512\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mnum_heads = 8\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22min_proj_weight = Parameter containing:Parameter[1536, 512] n=786432 (3Mb) x∈[-0.054, 0.054] μ=-1.519e-05 σ=0.031 grad\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22min_proj_bias = Parameter containing:Parameter[1536] 6Kb all_zeros grad\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mbias_k = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mbias_v = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22madd_zero_attn = False\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mdropout_p = 0.0\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mout_proj_weight = Parameter containing:Parameter[512, 512] n=262144 (1Mb) x∈[-0.044, 0.044] μ=3.247e-05 σ=0.026 grad\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mout_proj_bias = Parameter containing:Parameter[512] 2Kb all_zeros grad\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mtraining = True\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mkey_padding_mask = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mneed_weights = True\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mattn_mask = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22muse_separate_proj_weight = False\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mq_proj_weight = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mk_proj_weight = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mv_proj_weight = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mstatic_k = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mstatic_v = None\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22maverage_attn_weights = True\u001b[0m\n",
            "\u001b[32m\u001b[2mStarting var:.. \u001b[22mis_causal = False\u001b[0m\n",
            "\u001b[2m10:25:14.381664 call      5130\u001b[0m def multi_head_attention_forward(\n",
            "\u001b[2m10:25:14.390443 line      5234\u001b[0m     tens_ops = (query, key, value, in_proj_weight, in_proj_bias, bias_k, bias_v, out_proj_weight, out_proj_bias)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mtens_ops = (tensor[5, 3, 512] n=7680 (30Kb) x∈[-3.828, 3.95...ter containing:Parameter[512] 2Kb all_zeros grad)\u001b[0m\n",
            "\u001b[2m10:25:14.401395 line      5235\u001b[0m     if has_torch_function(tens_ops):\n",
            "\u001b[2m10:25:14.416205 line      5266\u001b[0m     is_batched = _mha_shape_check(query, key, value, key_padding_mask, attn_mask, num_heads)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mis_batched = True\u001b[0m\n",
            "\u001b[2m10:25:14.433508 line      5271\u001b[0m     if not is_batched:\n",
            "\u001b[2m10:25:14.450198 line      5280\u001b[0m     tgt_len, bsz, embed_dim = query.shape\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mtgt_len = 5\u001b[0m\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mbsz = 3\u001b[0m\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22membed_dim = 512\u001b[0m\n",
            "\u001b[2m10:25:14.466113 line      5281\u001b[0m     src_len, _, _ = key.shape\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22msrc_len = 5\u001b[0m\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22m_ = 512\u001b[0m\n",
            "\u001b[2m10:25:14.483400 line      5283\u001b[0m     key_padding_mask = _canonical_mask(\n",
            "\u001b[2m10:25:14.499779 line      5284\u001b[0m         mask=key_padding_mask,\n",
            "\u001b[2m10:25:14.516903 line      5285\u001b[0m         mask_name=\"key_padding_mask\",\n",
            "\u001b[2m10:25:14.532274 line      5286\u001b[0m         other_type=_none_or_dtype(attn_mask),\n",
            "\u001b[2m10:25:14.547479 line      5287\u001b[0m         other_name=\"attn_mask\",\n",
            "\u001b[2m10:25:14.562558 line      5288\u001b[0m         target_type=query.dtype\n",
            "\u001b[2m10:25:14.577457 line      5283\u001b[0m     key_padding_mask = _canonical_mask(\n",
            "\u001b[2m10:25:14.591818 line      5291\u001b[0m     if is_causal and attn_mask is None:\n",
            "\u001b[2m10:25:14.611160 line      5298\u001b[0m     if is_causal and key_padding_mask is None and not need_weights:\n",
            "\u001b[2m10:25:14.625571 line      5304\u001b[0m         attn_mask = _canonical_mask(\n",
            "\u001b[2m10:25:14.640381 line      5305\u001b[0m             mask=attn_mask,\n",
            "\u001b[2m10:25:14.657538 line      5306\u001b[0m             mask_name=\"attn_mask\",\n",
            "\u001b[2m10:25:14.681394 line      5307\u001b[0m             other_type=None,\n",
            "\u001b[2m10:25:14.699401 line      5308\u001b[0m             other_name=\"\",\n",
            "\u001b[2m10:25:14.714471 line      5309\u001b[0m             target_type=query.dtype,\n",
            "\u001b[2m10:25:14.729826 line      5310\u001b[0m             check_other=False,\n",
            "\u001b[2m10:25:14.744814 line      5304\u001b[0m         attn_mask = _canonical_mask(\n",
            "\u001b[2m10:25:14.759766 line      5313\u001b[0m         if key_padding_mask is not None:\n",
            "\u001b[2m10:25:14.774798 line      5319\u001b[0m     assert embed_dim == embed_dim_to_check, \\\n",
            "\u001b[2m10:25:14.790121 line      5321\u001b[0m     if isinstance(embed_dim, torch.Tensor):\n",
            "\u001b[2m10:25:14.804035 line      5325\u001b[0m         head_dim = embed_dim // num_heads\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mhead_dim = 64\u001b[0m\n",
            "\u001b[2m10:25:14.823453 line      5326\u001b[0m     assert head_dim * num_heads == embed_dim, f\"embed_dim {embed_dim} not divisible by num_heads {num_heads}\"\n",
            "\u001b[2m10:25:14.839186 line      5327\u001b[0m     if use_separate_proj_weight:\n",
            "\u001b[2m10:25:14.854018 line      5332\u001b[0m         assert key.shape == value.shape, f\"key shape {key.shape} does not match value shape {value.shape}\"\n",
            "\u001b[2m10:25:14.868675 line      5337\u001b[0m     if not use_separate_proj_weight:\n",
            "\u001b[2m10:25:14.884306 line      5338\u001b[0m         assert in_proj_weight is not None, \"use_separate_proj_weight is False but in_proj_weight is None\"\n",
            "\u001b[2m10:25:14.898646 line      5339\u001b[0m         q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mq = tensor[5, 3, 512] n=7680 (30Kb) x∈[-2.724, 2.979] μ=-0.003 σ=0.711 grad SelectBackward0\u001b[0m\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mk = tensor[5, 3, 512] n=7680 (30Kb) x∈[-2.555, 2.712] μ=-0.008 σ=0.700 grad SelectBackward0\u001b[0m\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mv = tensor[5, 3, 512] n=7680 (30Kb) x∈[-2.527, 2.571] μ=0.000 σ=0.706 grad SelectBackward0\u001b[0m\n",
            "\u001b[2m10:25:14.915553 line      5352\u001b[0m     if attn_mask is not None:\n",
            "\u001b[2m10:25:14.934271 line      5367\u001b[0m     if bias_k is not None and bias_v is not None:\n",
            "\u001b[2m10:25:14.951173 line      5377\u001b[0m         assert bias_k is None\n",
            "\u001b[2m10:25:14.967420 line      5378\u001b[0m         assert bias_v is None\n",
            "\u001b[2m10:25:14.989366 line      5383\u001b[0m     q = q.view(tgt_len, bsz * num_heads, head_dim).transpose(0, 1)\n",
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mq = tensor[24, 5, 64] n=7680 (30Kb) x∈[-2.724, 2.979] μ=-0.003 σ=0.711 grad TransposeBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.007213 line      5384\u001b[0m     if static_k is None:\n",
            "\u001b[2m10:25:15.026460 line      5385\u001b[0m         k = k.view(k.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mk = tensor[24, 5, 64] n=7680 (30Kb) x∈[-2.555, 2.712] μ=-0.008 σ=0.700 grad TransposeBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.046454 line      5393\u001b[0m     if static_v is None:\n",
            "\u001b[2m10:25:15.062271 line      5394\u001b[0m         v = v.view(v.shape[0], bsz * num_heads, head_dim).transpose(0, 1)\n",
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mv = tensor[24, 5, 64] n=7680 (30Kb) x∈[-2.527, 2.571] μ=0.000 σ=0.706 grad TransposeBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.079174 line      5404\u001b[0m     if add_zero_attn:\n",
            "\u001b[2m10:25:15.094769 line      5414\u001b[0m     src_len = k.size(1)\n",
            "\u001b[2m10:25:15.110781 line      5417\u001b[0m     if key_padding_mask is not None:\n",
            "\u001b[2m10:25:15.127052 line      5428\u001b[0m     if not training:\n",
            "\u001b[2m10:25:15.144089 line      5435\u001b[0m     if need_weights:\n",
            "\u001b[2m10:25:15.160736 line      5436\u001b[0m         B, Nt, E = q.shape\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mB = 24\u001b[0m\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mNt = 5\u001b[0m\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mE = 64\u001b[0m\n",
            "\u001b[2m10:25:15.178967 line      5437\u001b[0m         q_scaled = q / math.sqrt(E)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mq_scaled = tensor[24, 5, 64] n=7680 (30Kb) x∈[-0.341, 0.372] μ=-0.000 σ=0.089 grad DivBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.195304 line      5439\u001b[0m         assert not (is_causal and attn_mask is None), \"FIXME: is_causal not implemented for need_weights\"\n",
            "\u001b[2m10:25:15.211920 line      5441\u001b[0m         if attn_mask is not None:\n",
            "\u001b[2m10:25:15.232614 line      5444\u001b[0m             attn_output_weights = torch.bmm(q_scaled, k.transpose(-2, -1))\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mattn_output_weights = tensor[24, 5, 5] n=600 (2.3Kb) x∈[-1.804, 1.412] μ=0.003 σ=0.491 grad BmmBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.252828 line      5445\u001b[0m         attn_output_weights = softmax(attn_output_weights, dim=-1)\n",
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mattn_output_weights = tensor[24, 5, 5] n=600 (2.3Kb) x∈[0.037, 0.542] μ=0.200 σ=0.085 grad SoftmaxBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.270913 line      5446\u001b[0m         if dropout_p > 0.0:\n",
            "\u001b[2m10:25:15.287280 line      5449\u001b[0m         attn_output = torch.bmm(attn_output_weights, v)\n",
            "\u001b[32m\u001b[2mNew var:....... \u001b[22mattn_output = tensor[24, 5, 64] n=7680 (30Kb) x∈[-1.463, 1.361] μ=-0.001 σ=0.346 grad BmmBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.309268 line      5451\u001b[0m         attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len * bsz, embed_dim)\n",
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mattn_output = tensor[15, 512] n=7680 (30Kb) x∈[-1.463, 1.361] μ=-0.001 σ=0.346 grad ViewBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.327969 line      5452\u001b[0m         attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n",
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mattn_output = tensor[15, 512] n=7680 (30Kb) x∈[-0.833, 0.785] μ=-0.001 σ=0.205 grad AddmmBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.346426 line      5453\u001b[0m         attn_output = attn_output.view(tgt_len, bsz, attn_output.size(1))\n",
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mattn_output = tensor[5, 3, 512] n=7680 (30Kb) x∈[-0.833, 0.785] μ=-0.001 σ=0.205 grad ViewBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.364293 line      5456\u001b[0m         attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n",
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mattn_output_weights = tensor[3, 8, 5, 5] n=600 (2.3Kb) x∈[0.037, 0.542] μ=0.200 σ=0.085 grad ViewBackward0\u001b[0m\n",
            "\u001b[2m10:25:15.388394 line      5457\u001b[0m         if average_attn_weights:\n",
            "\u001b[2m10:25:15.407016 line      5458\u001b[0m             attn_output_weights = attn_output_weights.mean(dim=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor[5, 3, 512] n=7680 (30Kb) x∈[-3.756, 3.655] μ=-0.005 σ=1.034 grad AddBackward0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[32m\u001b[2mModified var:.. \u001b[22mattn_output_weights = tensor[3, 5, 5] n=75 x∈[0.132, 0.308] μ=0.200 σ=0.036 grad MeanBackward1\u001b[0m\n",
            "\u001b[2m10:25:15.425229 line      5460\u001b[0m         if not is_batched:\n",
            "\u001b[2m10:25:15.447279 line      5464\u001b[0m         return attn_output, attn_output_weights\n",
            "\u001b[2m10:25:15.464976 return    5464\u001b[0m         return attn_output, attn_output_weights\n",
            "\u001b[36m\u001b[2mReturn value:.. \u001b[22m(tensor[5, 3, 512] n=7680 (30Kb) x∈[-0.833, 0.78...0.132, 0.308] μ=0.200 σ=0.036 grad MeanBackward1)\u001b[0m\n",
            "\u001b[33m\u001b[2mElapsed time: \u001b[22m00:00:01.104722\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JB96SBQhNR0w"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
